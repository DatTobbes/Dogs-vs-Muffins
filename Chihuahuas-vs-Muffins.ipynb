{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional Neural Network Tutorial\n",
    "\n",
    "This notebook offers a first simple introduction to the use of Convolutional Neural Networks. All important steps necessary to load training data, create and train a model and evaluate it are performed and described.\n",
    "\n",
    "A CNN will be trained as a binary classifier to distinguish Chihuahuas from muffins.  \n",
    "<img src=\"Notebook_Images/full.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a function\n",
    "\n",
    "With Jupyter, function can be defined and reused elsewhere. In addition, two variables (image_height, image_width) are defined in this code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_height=172\n",
    "image_width= 171\n",
    "filename= 'your.npz'\n",
    "\n",
    "def plot(images_to_plot, predictions= np.array([]), titles=[]):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(images_to_plot)):\n",
    "        ax = fig.add_subplot(4, 4, 1 + i, xticks=[], yticks=[])\n",
    "        im = images_to_plot[i]\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "        if predictions.size:\n",
    "            ax.set_xlabel('dog: %6.2f \\n muffin %6.2f'\n",
    "                      % (predictions[i][0], predictions[i][1]), fontsize=12)\n",
    "        plt.imshow(im)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load image data\n",
    "First the data must be loaded for the training. The individual files must be sorted by class and placed in the corresponding folders, as shown below.  <br>\n",
    "* \\Data\n",
    "   * \\Chihuahuas \n",
    "      *  img1.jpg\n",
    "      *  img2.jpg\n",
    "      *  img3.jpg \n",
    "   * \\Muffins\n",
    "       * img1.jpg \n",
    "       * img2.jpg \n",
    "       * img3.jpg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset as a numpy .npz\n",
    "To avoid loading the images again and again, numpy provides a very efficient method to save large amounts of data as a .npz file. To do this, the images and labels must be passed to the method as numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2b0d743069fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m np.savez_compressed(filename,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     labels=labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.savez_compressed(filename,\n",
    "                    images=images,\n",
    "                    labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images from .npz file\n",
    "numpy also provides a function to load data from an npz file. The images and labels are read as numpy arrays from the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset= np.load(file=filename)\n",
    "labels=dataset['labels']\n",
    "images=dataset['images']\n",
    "images= np.concatenate((images[labels ==0][:100], images[labels ==1][:100]) ,axis=0)\n",
    "labels= np.concatenate((labels[labels ==0][:100], labels[labels ==1][:100]) ,axis=0)\n",
    "print('load %d files' %len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show some testimages\n",
    "Now the plot method, which we defined at the beginning, is used to display the images. In the first example eight images of each class are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_to_show= np.concatenate((images[labels ==0][:8], images[labels ==1][:8]) ,axis=0)\n",
    "print(len(images_to_show))\n",
    "\n",
    "plot(images_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation\n",
    "To increase the amount of test data, the images have to be processed. For this example, the images are randomly flipped vertically or horizontally. In addition, a copy of the original image is created as a gray value image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img= images[0]\n",
    "\n",
    "horizontal_flipt= cv2.flip(img, 0)\n",
    "vertical_flipt= cv2.flip(img, 1)\n",
    "gray_scaled_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "augmented_images= [img, horizontal_flipt, vertical_flipt, gray_scaled_img]\n",
    "titels=['original', 'horizonzal', 'vertical', 'grayscaled']\n",
    "plot(augmented_images, titles=titels)\n",
    "\n",
    "def randomize_images(images,labels):\n",
    "\n",
    "    for index in range(len(images)):\n",
    "        img = images[index]\n",
    "        flipt_img= cv2.flip(img, np.random.randint(0,1))\n",
    "        flipt_img = np.expand_dims(flipt_img, axis=0)\n",
    "        images=np.vstack((images, flipt_img))\n",
    "        labels = np.append(labels, np.asarray(labels[index]))\n",
    "    print(len(images))\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing the data, the arrays are randomly mixed. The sklearn.shuffel method shuffles the images and labels equally. Set the Random_State for reproducibility  to a constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "images , labels = shuffle(images , labels, random_state=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that the date are divided into training, validation and test samples. \n",
    "Training and validation samples are used to adjust weights during learning. \n",
    "The test samples have to be split off before the training phase and are used to evaluate the classifier after training. \n",
    "\n",
    "<img src=\"Notebook_Images/Test-train-Val.PNG\" height=\"600\" width=\"600\" >\n",
    "<cite data-cite=\"Raschka\">(Sebastian Raschka und Vahid Mirjalili. Machine Learning mit Python\n",
    "und Scikit-learn und TensorFlow: Das umfassende Praxis-Handbuch für\n",
    "Data Science, Deep Learning und Predictive Analytics. 2., aktualisierte\n",
    "und erweiterte Auflage. Frechen: mitp, 2018. isbn: 978-3-95845-733-1.)</cite>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "length=len(labels)\n",
    "\n",
    "X_train = images[:int(length*0.8)]\n",
    "X_val   = images[int(length*0.8):int(length*0.9)]\n",
    "X_test  = images[int(length*0.9):]\n",
    "y_train = labels[:int(length*0.8)] \n",
    "y_val   = labels[int(length*0.8):int(length*0.9)]\n",
    "y_test  = labels[int(length*0.9):]\n",
    "\n",
    "\n",
    "y_train= np_utils.to_categorical(y_train, 2)\n",
    "y_val = np_utils.to_categorical(y_val, 2)\n",
    "\n",
    "print(\"Trainingsdaten:\", X_train.shape)\n",
    "print(\"Validierungsdaten:\", X_val.shape)\n",
    "print(\"Testdaten:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of feature values\n",
    "To prevent outliers in the pixel values from having a large influence on the weight change, the feature values are standardized. \n",
    "\n",
    "\\begin{equation*}\n",
    "x^{(i)}_{std}=\\frac{x^{(i)}-\\mu_x}{\\sigma_x}\n",
    "\\end{equation*}\n",
    "\n",
    "<cite data-cite=\"Raschka\">(Sebastian Raschka und Vahid Mirjalili. Machine Learning mit Python\n",
    "und Scikit-learn und TensorFlow: Das umfassende Praxis-Handbuch für\n",
    "Data Science, Deep Learning und Predictive Analytics. 2., aktualisierte\n",
    "und erweiterte Auflage. Frechen: mitp, 2018. isbn: 978-3-95845-733-1.)</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_values=np.mean(X_train, axis=0)\n",
    "std_values=np.std(X_train)\n",
    "\n",
    "x_train_centered= (X_train- mean_values)/std_values\n",
    "x_val_centered= (X_val- mean_values)/std_values\n",
    "x_test_centered= (X_test- mean_values)/std_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "This command creates a convolutional layer in the model. For this the number of filters (32) and the size of the filter kernels in pixel(5x5) as well as the activation function activation('relu') must be specified. Optionally the initialization and the padding mode can be specified.   \n",
    "    * model.add(Convolution2D(32, (5, 5), activation='relu',kernel_initializer='glorot_uniform', padding='same'))\n",
    "    \n",
    "After a convolution block a pooling operation follows. The pooling layer in this example has the size 2x2 pixels.\n",
    "    * model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "Dense layers are fully connected layers. Here in the example with 512 neurons on this layer. The number of neurons in the last dense layer determines the number of classes to be distinguished.\n",
    "    * model.add(Dense(512,kernel_initializer='glorot_uniform', activation='relu'))\n",
    "    \n",
    "Finally, the model has to be compiled. In this case, Categorical-Crossentropy is used as the loss function, Binaray-Crossentropy can be used for a two-class problem. As optimization method stochhastical gradient descent (SGD) is specified, the parameters such as learning rate (lr) can be adjusted during learning.  \n",
    "    * model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "    \n",
    "With the model.summary command the structure of the model can be printed in the shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D,  BatchNormalization, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, (5, 5), activation='relu',kernel_initializer='glorot_uniform', padding='same', \n",
    "                        input_shape=(image_height, image_width, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(32, (3, 3),kernel_initializer='glorot_uniform', activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(32, (3, 3),kernel_initializer='glorot_uniform', activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,kernel_initializer='glorot_uniform', activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128,kernel_initializer='glorot_uniform', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy',  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The Fit command trains the model. For this, training and validation data as well as the number of training epochs must be passed on. The verbose parameter is be used to determine how often a shell output with current training values should be printed during training. Callbacks accepts a list of predefined callback methods, in our case a callback has been defined which stores the current weightings during the training as soon as the accuracy has increased in an epoch and the loss has fallen. Callbacks can also be defined to store certain values for later visualization or to adapt the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "checkpointer = ModelCheckpoint(filepath='model.hdf5', verbose=1, save_best_only=True)\n",
    "earlyStopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "model.fit(x_train_centered, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val_centered,  y_val),\n",
    "          verbose=1,\n",
    "          callbacks=[checkpointer,earlyStopper, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a trained model\n",
    "For later reuse, models can be saved during and after the learning process. Keras therefore offers several formats like JSON or hdf5. These models can then be reloaded to classify new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The Predict method is used to predict the class of images with the model. To do this, individual images or an array of images can be passed to the method.  \n",
    "\n",
    "The evaluate-function is a fast function to evaluate a model. This method calculates accuracy and loss for an array of data.  \n",
    "\n",
    "At the beginning, the images were divided into training, validation and test samples. The test sample was not used during training and is therefore still unknown to the model. Thus it can be used to evaluate the model.\n",
    "\n",
    "The class probabilities for the actual productive data are then computed. To do this, they are first loaded and standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_plot= X_test[:16]\n",
    "predictions= model.predict(x_test_centered)[:16]\n",
    "plot(img_to_plot, predictions= predictions)\n",
    "\n",
    "true_labels=np_utils.to_categorical(y_test, 2)\n",
    "metric= model.evaluate(x_test_centered, true_labels)\n",
    "\n",
    "print('loss: %10.2f  \\naccuracy: %6.2f' % (metric[0], metric[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= np.load('test_data.npz')\n",
    "labels=dataset['labels']\n",
    "images=dataset['images']\n",
    "\n",
    "mean_values=np.mean(X_train, axis=0)\n",
    "std_values=np.std(X_train)\n",
    "\n",
    "x_test_centered= (images- mean_values)/std_values\n",
    "plot(images, predictions= model.predict(x_test_centered))\n",
    "\n",
    "true_labels=np_utils.to_categorical(labels, 2)\n",
    "metric= model.evaluate(x_test_centered, true_labels)\n",
    "\n",
    "print('loss: %10.2f  \\naccuracy: %6.2f' % (metric[0], metric[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
